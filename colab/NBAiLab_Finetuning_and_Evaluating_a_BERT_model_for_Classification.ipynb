{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NBAiLab - Finetuning and Evaluating a BERT model for Classification",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mCOuP-rezRoW"
      },
      "source": [
        "# NBAiLab - Finetuning and Evaluating a BERT model for Classification\n",
        "<img src=\"https://raw.githubusercontent.com/NBAiLab/notram/master/images/nblogo_2.png\">\n",
        "\n",
        "\n",
        "In this notebook we will finetune the [NB-BERTbase Model](https://github.com/NBAiLab/notram) released by the National Library of Norway. This is a model trained on a large corpus (110GB) of Norwegian texts. \n",
        "\n",
        "We will finetune this model on a sentiment classification task based on the [NoReC: The Norwegian Review Corpus](https://github.com/ltgoslo/norec). By simply replacing the dataset, you should be able to use this code to train any classifier.\n",
        "\n",
        "After training, we will save the model, evaluate it and use it for predictions.\n",
        "\n",
        "The Notebook is intended for experimentation with the pre-release NoTram models from the National Library of Norway, and is made for educational purposes. If you just want to use the model, you can instead initiate one of our finetuned models. \n",
        "\n",
        "## Before proceeding\n",
        "Create a copy of this notebook by going to \"File - Save a Copy in Drive\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4evaajXr0uH5"
      },
      "source": [
        "!pip install transformers\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import json\n",
        "import math\n",
        "from transformers import BertTokenizer, AutoConfig, TFAutoModelForSequenceClassification, optimization_tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2oOQn5iq_ZG1"
      },
      "source": [
        "# Settings\n",
        "Try running this with the default settings first. The default setting should give you a pretty good result. You can then experiment with the other settings to get even better results. A warmup around 10% usually give you more stable results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5D21tLr9_n1M"
      },
      "source": [
        "#@markdown Set the main model that the training should start from\n",
        "model_name = 'NbAiLab/nb-bert-base' #@param [\"NbAiLab/nb-bert-base\", \"bert-base-multilingual-cased\"]\n",
        "#@markdown ---\n",
        "#@markdown Set training parameters\n",
        "batch_size =  8#@param {type: \"integer\"} \n",
        "init_lr = 3e-5 #@param {type: \"number\"}\n",
        "end_lr = 0  #@param {type: \"number\"}\n",
        "num_warmup_steps = 300 #@param {type: \"number\"}\n",
        "num_epochs =   4#@param {type: \"integer\"}\n",
        "max_seq_length = 512"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vsn8gblCARHi"
      },
      "source": [
        "# Load and Prepare the Dataset used for Finetuning\n",
        "The current dataset is loaded directly from a web resource. It is coded for positive/negative sentiment (1/0) and is in a csv-file. You can replace this with any other data source. This data is converted into tensor slices that Tensorflow needs. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4utMn85m12vB"
      },
      "source": [
        "train_data = pd.read_csv ('https://raw.githubusercontent.com/ltgoslo/NorBERT/main/benchmarking/data/sentiment/no/train.csv', header = None)\n",
        "dev_data = pd.read_csv ('https://raw.githubusercontent.com/ltgoslo/NorBERT/main/benchmarking/data/sentiment/no/dev.csv', header = None)\n",
        "test_data = pd.read_csv ('https://raw.githubusercontent.com/ltgoslo/NorBERT/main/benchmarking/data/sentiment/no/test.csv', header = None)\n",
        "\n",
        "\n",
        "#Initialize the tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "\n",
        "#Turn text into tokens\n",
        "train_encodings = tokenizer(list(train_data[1]), truncation=True, padding=True, max_length=max_seq_length)\n",
        "dev_encodings = tokenizer(list(dev_data[1]), truncation=True, padding=True, max_length=max_seq_length)\n",
        "test_encodings = tokenizer(list(test_data[1]), truncation=True, padding=True, max_length=max_seq_length)\n",
        "\n",
        "#Create a tensorflow dataset\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((dict(train_encodings),list(train_data[0])))\n",
        "dev_dataset = tf.data.Dataset.from_tensor_slices((dict(dev_encodings),list(dev_data[0])))\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((dict(test_encodings),list(test_data[0])))\n",
        "\n",
        "\n",
        "print(f'The dataset is imported.\\n\\nThe training dataset has {len(train_dataset)} items.\\nThe development dataset has {len(dev_dataset)} items. \\nThe test dataset has {len(test_dataset)} items')\n",
        "steps = math.ceil(len(train_dataset)/batch_size)\n",
        "print(f'You are planning to train for a total of {steps} steps * {num_epochs} epochs = {num_epochs*steps} steps. Warmup is {num_warmup_steps}, {int(100*num_warmup_steps/(steps*num_epochs))}%. We recommend at least 10%.')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vf2ZIk4t-_38"
      },
      "source": [
        "# Start Training\n",
        "We are here using the Tensorflow interface provided by Huggingface. Huggingface also has a native interface as well as one for PyTorch. To see an example of how to use the native interface, please take a look at our notebook about NER/POS."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e54Hfg_-12jP"
      },
      "source": [
        "#Estimate the number of training steps\n",
        "train_steps_per_epoch = int(len(train_dataset)/batch_size)\n",
        "num_train_steps = train_steps_per_epoch * num_epochs\n",
        "\n",
        "# Initialise a Model for Sequence Classification with 2 labels\n",
        "config = AutoConfig.from_pretrained(model_name, num_labels=2)\n",
        "model = TFAutoModelForSequenceClassification.from_pretrained(model_name, config=config)\n",
        "\n",
        "#Creating a scheduler gives us a bit more control\n",
        "optimizer, lr_schedule = optimization_tf.create_optimizer(init_lr=init_lr, num_train_steps=num_train_steps, num_warmup_steps=num_warmup_steps)\n",
        "\n",
        "#Compile the model\n",
        "model.compile(optimizer=optimizer, loss=model.compute_loss, metrics=['accuracy']) # can also use any keras loss fn\n",
        "\n",
        "#Start training\n",
        "history = model.fit(train_dataset.shuffle(1000).batch(batch_size), validation_data=dev_dataset.shuffle(1000).batch(batch_size), epochs=num_epochs, batch_size=batch_size)\n",
        "\n",
        "print(f'\\nThe training has finished training after {num_epochs} epochs.')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-kWbNMOELJYr"
      },
      "source": [
        "# Run Preditions on the Test Dataset\n",
        "When you have finished the training and are satisfied with the result, you are ready to see how the model works on your test set. Typically you would save your model first, and load it again. However, this is explained in our NER/POS notebook, so we are skipping this to make the notebook shorter. \n",
        "\n",
        "Here we show how you can calculate F1-score (also called \"Macro average\") you want to report.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qh2dYNY2ETf_"
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "y_pred = model.predict(test_encodings['input_ids'])\n",
        "y_pred_bool = np.argmax(y_pred['logits'], axis=1)\n",
        "\n",
        "print(classification_report(test_data[0], y_pred_bool,digits=4))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}